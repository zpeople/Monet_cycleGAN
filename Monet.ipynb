{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GANs to create art - will you be the next Monet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "github: https://github.com/zpeople/Monet_cycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:48:39.411750Z",
     "iopub.status.busy": "2024-12-13T04:48:39.410908Z",
     "iopub.status.idle": "2024-12-13T04:48:52.454653Z",
     "shell.execute_reply": "2024-12-13T04:48:52.453935Z",
     "shell.execute_reply.started": "2024-12-13T04:48:39.411696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers ,Model\n",
    "from tensorflow.keras.layers import Dense, Flatten,Reshape,BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:48:52.456388Z",
     "iopub.status.busy": "2024-12-13T04:48:52.455920Z",
     "iopub.status.idle": "2024-12-13T04:48:53.048950Z",
     "shell.execute_reply": "2024-12-13T04:48:53.047967Z",
     "shell.execute_reply.started": "2024-12-13T04:48:52.456359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "  \n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    \n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:  \n",
    "    print(\"No TPU found, using default strategy.\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            \n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            print(\"Using MirroredStrategy for GPU distribution.\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print(\"No GPU found, using default strategy.\")\n",
    "\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFRecord is a binary file format used by TensorFlow for efficient storage of large data sets. Instead of storing data directly in a human-readable format, it serializes the data into a compact, continuous format, which makes TFRecord files ideal for use in input pipelines, especially when large data sets need to be processed.\n",
    "\n",
    "A TFRecord file consists of a series of records, each of which is a string, with the following internal structure:\n",
    "\n",
    "Length Prefix: A VARint-encoded integer indicating the byte length of the next part of the data.\n",
    "Data: Actual data, which can be any byte stream.\n",
    "CRC-32C Checksum (CRC-32C Checksum) : Another Varint-encoded integer containing the CRC-32C checksum, used to verify that parts of the data were corrupted during transmission or storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "The dataset contains four directories: monet_tfrec, photo_tfrec, monet_jpg, and photo_jpg. The monet_tfrec and monet_jpg directories contain the same painting images, and the photo_tfrec and photo_jpg directories contain the same photos.\n",
    "\n",
    "* monet_jpg - 300 Monet paintings sized 256x256 in JPEG format\n",
    "* monet_tfrec - 300 Monet paintings sized 256x256 in TFRecord format\n",
    "* photo_jpg - 7028 photos sized 256x256 in JPEG format\n",
    "* photo_tfrec - 7028 photos sized 256x256 in TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:48:53.050536Z",
     "iopub.status.busy": "2024-12-13T04:48:53.050270Z",
     "iopub.status.idle": "2024-12-13T04:48:53.084727Z",
     "shell.execute_reply": "2024-12-13T04:48:53.083893Z",
     "shell.execute_reply.started": "2024-12-13T04:48:53.050509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Defining the directories for the image locations\n",
    "\n",
    "INPUT_PATH='Datasets'\n",
    "OUT_PUT_PATH ='./'\n",
    "\n",
    "#INPUT_PATH='/kaggle/input/gan-getting-started'\n",
    "#OUT_PUT_PATH ='/kaggle/working/'\n",
    "\n",
    "monet_jpegs = INPUT_PATH+\"/monet_jpg\"\n",
    "photos_jpegs = INPUT_PATH+\"/photo_jpg\"\n",
    "monet_tfrec = INPUT_PATH+\"/monet_tfrec\"\n",
    "photos_tfrec = INPUT_PATH+\"/photo_tfrec\"\n",
    "MONET_FILENAMES = tf.io.gfile.glob(str(monet_tfrec+'/*.tfrec'))\n",
    "print('Monet TFRecord Files:', len(MONET_FILENAMES))\n",
    "\n",
    "PHOTO_FILENAMES = tf.io.gfile.glob(str(photos_tfrec + '/*.tfrec'))\n",
    "print('Photo TFRecord Files:', len(PHOTO_FILENAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load TFRecord to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:48:53.087630Z",
     "iopub.status.busy": "2024-12-13T04:48:53.087018Z",
     "iopub.status.idle": "2024-12-13T04:48:53.226561Z",
     "shell.execute_reply": "2024-12-13T04:48:53.225935Z",
     "shell.execute_reply.started": "2024-12-13T04:48:53.087602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [256, 256]\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS =20\n",
    "def tfrec2image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = {\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = tfrec2image(example['image'])\n",
    "    return image\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "monet_data = load_dataset(MONET_FILENAMES)\n",
    "photo_data = load_dataset(PHOTO_FILENAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:48:53.227719Z",
     "iopub.status.busy": "2024-12-13T04:48:53.227468Z",
     "iopub.status.idle": "2024-12-13T04:48:53.287799Z",
     "shell.execute_reply": "2024-12-13T04:48:53.286978Z",
     "shell.execute_reply.started": "2024-12-13T04:48:53.227694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "monet_data = monet_data.repeat()\n",
    "photo_data = photo_data.repeat()\n",
    "    \n",
    "monet_data = monet_data.shuffle(2048)\n",
    "photo_data = photo_data.shuffle(2048)\n",
    "\n",
    "def augment(image):\n",
    "    return tf.image.random_flip_left_right(image)\n",
    "\n",
    "monet_data = monet_data.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "photo_data = photo_data.map(augment, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:48:53.289226Z",
     "iopub.status.busy": "2024-12-13T04:48:53.288879Z",
     "iopub.status.idle": "2024-12-13T04:48:59.852047Z",
     "shell.execute_reply": "2024-12-13T04:48:59.851181Z",
     "shell.execute_reply.started": "2024-12-13T04:48:53.289188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for batch_monet_images in monet_data.take(1):  \n",
    "    print(\"Shape of a batch of images:\", batch_monet_images.shape)\n",
    "for batch_photo_images in photo_data.take(1):  \n",
    "    print(\"Shape of a batch of images:\", batch_photo_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:48:59.853646Z",
     "iopub.status.busy": "2024-12-13T04:48:59.853351Z",
     "iopub.status.idle": "2024-12-13T04:49:00.164607Z",
     "shell.execute_reply": "2024-12-13T04:49:00.163799Z",
     "shell.execute_reply.started": "2024-12-13T04:48:59.853618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title('Photo')\n",
    "plt.imshow(batch_photo_images[0] * 0.5 + 0.5)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Monet')\n",
    "plt.imshow(batch_monet_images[0] * 0.5 + 0.5)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:00.166492Z",
     "iopub.status.busy": "2024-12-13T04:49:00.165861Z",
     "iopub.status.idle": "2024-12-13T04:49:00.178812Z",
     "shell.execute_reply": "2024-12-13T04:49:00.177924Z",
     "shell.execute_reply.started": "2024-12-13T04:49:00.166438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class InstanceNormalization(layers.Layer):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale',\n",
    "            shape=input_shape[-1:],\n",
    "            initializer=\"ones\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.offset = self.add_weight(\n",
    "            name='offset',\n",
    "            shape=input_shape[-1:],\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n",
    "        return self.scale * (inputs - mean) / tf.sqrt(variance + self.epsilon) + self.offset\n",
    "\n",
    "# filter-The number of filters in the Conv2D layer is the number of output feature maps\n",
    "# size -Convolution kernel size\n",
    "def Cov2d(filters, size, apply_instancenorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = Sequential()\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "    if apply_instancenorm:\n",
    "        instance_norm_layer = InstanceNormalization()\n",
    "        result.add(instance_norm_layer)\n",
    "    result.add(layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "\n",
    "def deCov2d(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = Sequential()\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "    instance_norm_layer = InstanceNormalization()\n",
    "    result.add(instance_norm_layer)\n",
    "    if apply_dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "    result.add(layers.ReLU())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of down-sampling and up-sampling enables deep learning models to extract and recover features efficiently, while reducing computational complexity and enhancing the model's ability to process features at different scales.\n",
    "\n",
    "Downsampling:\n",
    "Each layer continues to use a 4x4 convolution kernel with a step size of 2, gradually reducing the space size and increasing the number of channels until the final layer reaches the maximum number of channels of 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:00.180257Z",
     "iopub.status.busy": "2024-12-13T04:49:00.179911Z",
     "iopub.status.idle": "2024-12-13T04:49:00.193291Z",
     "shell.execute_reply": "2024-12-13T04:49:00.192330Z",
     "shell.execute_reply.started": "2024-12-13T04:49:00.180217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def Generator():\n",
    "#     inputs  = layers.Input(shape=[256,256,3])\n",
    "    \n",
    "#     Downsampling =[\n",
    "#         Cov2d(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n",
    "#         Cov2d(128, 4), # (bs, 64, 64, 128)\n",
    "#         Cov2d(256, 4), # (bs, 32, 32, 256)\n",
    "#         Cov2d(512, 4), # (bs, 16, 16, 512)\n",
    "#         Cov2d(512, 4), # (bs, 8, 8, 512)\n",
    "#         Cov2d(512, 4), # (bs, 4, 4, 512)\n",
    "#         Cov2d(512, 4), # (bs, 2, 2, 512)\n",
    "#         Cov2d(512, 4), # (bs, 1, 1, 512)\n",
    "#     ]\n",
    "    \n",
    "#     Upsampling = [\n",
    "#         deCov2d(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "#         deCov2d(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "#         deCov2d(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "#         deCov2d(512, 4), # (bs, 16, 16, 1024)\n",
    "#         deCov2d(256, 4), # (bs, 32, 32, 512)\n",
    "#         deCov2d(128, 4), # (bs, 64, 64, 256)\n",
    "#         deCov2d(64, 4), # (bs, 128, 128, 128)\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "#     initializer = tf.random_normal_initializer(0., 0.02)\n",
    "#     last = layers.Conv2DTranspose(3, 4,\n",
    "#                                   strides=2,\n",
    "#                                   padding='same',\n",
    "#                                   kernel_initializer=initializer,\n",
    "#                                   activation='tanh')  # (bs, 256, 256, 3)\n",
    "\n",
    "#     x = inputs\n",
    "\n",
    "#     skips = []\n",
    "#     for down in Downsampling:\n",
    "#         x = down(x)\n",
    "#         skips.append(x)\n",
    "#     #The last element has been removed because it was already used during the initial decoder_stack phase\n",
    "#     skips = reversed(skips[:-1]) \n",
    "\n",
    "#     for up, skip in zip(Upsampling, skips):\n",
    "#         x = up(x)\n",
    "#         x = layers.Concatenate()([x, skip])\n",
    "\n",
    "#     x = last(x)\n",
    "\n",
    "#     return Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:00.196555Z",
     "iopub.status.busy": "2024-12-13T04:49:00.196270Z",
     "iopub.status.idle": "2024-12-13T04:49:00.207713Z",
     "shell.execute_reply": "2024-12-13T04:49:00.206868Z",
     "shell.execute_reply.started": "2024-12-13T04:49:00.196530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import  Conv2D, Conv2DTranspose, BatchNormalization, Add, Lambda\n",
    "\n",
    "\n",
    "def residual_block(x, filters):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([shortcut, x])\n",
    "    return LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "\n",
    "\n",
    "def Generator():\n",
    "    inputs =  layers.Input(shape=[256, 256, 3])\n",
    "    \n",
    "    Downsampling =[\n",
    "        Cov2d(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n",
    "        Cov2d(128, 4), # (bs, 64, 64, 128)\n",
    "        Cov2d(256, 4), # (bs, 32, 32, 256)\n",
    "        Cov2d(512, 4), # (bs, 16, 16, 512)\n",
    "        Cov2d(512, 4), # (bs, 8, 8, 512)\n",
    "        Cov2d(512, 4), # (bs, 4, 4, 512)\n",
    "        Cov2d(512, 4), # (bs, 2, 2, 512)\n",
    "        Cov2d(512, 4), # (bs, 1, 1, 512)\n",
    "    ]\n",
    "    \n",
    "    Upsampling = [\n",
    "        deCov2d(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "        deCov2d(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "        deCov2d(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "        deCov2d(512, 4), # (bs, 16, 16, 1024)\n",
    "        deCov2d(256, 4), # (bs, 32, 32, 512)\n",
    "        deCov2d(128, 4), # (bs, 64, 64, 256)\n",
    "        deCov2d(64, 4), # (bs, 128, 128, 128)\n",
    "    ]\n",
    "    \n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = Conv2DTranspose(3, 4,\n",
    "                           strides=2,\n",
    "                           padding='same',\n",
    "                           kernel_initializer=initializer,\n",
    "                           activation='tanh')  # (bs, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "    \n",
    "    skips = []\n",
    "    for down in Downsampling:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    for up, skip in zip(Upsampling, skips):\n",
    "        x = up(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "        x = residual_block(x, x.shape[-1])  # Apply a residual block after each upsampling step\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:00.209721Z",
     "iopub.status.busy": "2024-12-13T04:49:00.208973Z",
     "iopub.status.idle": "2024-12-13T04:49:00.222034Z",
     "shell.execute_reply": "2024-12-13T04:49:00.221271Z",
     "shell.execute_reply.started": "2024-12-13T04:49:00.209680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inputs = layers.Input(shape=[256, 256, 3])  # 输入图像尺寸为 256x256，3 个通道（RGB）\n",
    "\n",
    "  \n",
    "    encoder_stack = [\n",
    "        Cov2d(64, 4, apply_instancenorm=False),  # (bs, 128, 128, 64)\n",
    "        Cov2d(128, 4),  # (bs, 64, 64, 128)\n",
    "        Cov2d(256, 4),  # (bs, 32, 32, 256)\n",
    "        Cov2d(512, 4),  # (bs, 16, 16, 512)\n",
    "    ]\n",
    "\n",
    "    \n",
    "    x = inputs\n",
    "\n",
    "  \n",
    "    for down in encoder_stack:\n",
    "        x = down(x)\n",
    "\n",
    "    # zero_pad1 = layers.ZeroPadding2D()(x)  #  (bs, 34, 34, 256)\n",
    "    # conv = layers.Conv2D(512, 4, strides=1,\n",
    "    #                      kernel_initializer=initializer,\n",
    "    #                      use_bias=False)(zero_pad1)  # 卷 (bs, 31, 31, 512)\n",
    "    # instance_norm_layer = InstanceNormalization()\n",
    "    # norm1 = instance_norm_layer(conv)  \n",
    "    # leaky_relu = layers.LeakyReLU()(norm1) \n",
    "\n",
    "    # zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
    "    # last = layers.Conv2D(1, 4, strides=1,\n",
    "    #                      kernel_initializer=initializer)(zero_pad2)  #  (bs, 30, 30, 1)\n",
    "    \n",
    "    last = layers.Conv2D(1, 4, strides=1,\n",
    "                         kernel_initializer=initializer)(x)#(bs, 13, 13, 1)\n",
    "    return Model(inputs=inputs, outputs=last)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:00.223387Z",
     "iopub.status.busy": "2024-12-13T04:49:00.223028Z",
     "iopub.status.idle": "2024-12-13T04:49:02.372196Z",
     "shell.execute_reply": "2024-12-13T04:49:02.371450Z",
     "shell.execute_reply.started": "2024-12-13T04:49:00.223346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    monet_generator = Generator() # transforms photos to Monet-esque paintings\n",
    "    photo_generator = Generator() # transforms Monet paintings to be more like photos\n",
    "\n",
    "    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n",
    "    photo_discriminator = Discriminator() # differentiates real photos and generated photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View feature extraction and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:02.373538Z",
     "iopub.status.busy": "2024-12-13T04:49:02.373261Z",
     "iopub.status.idle": "2024-12-13T04:49:06.724873Z",
     "shell.execute_reply": "2024-12-13T04:49:06.724025Z",
     "shell.execute_reply.started": "2024-12-13T04:49:02.373504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "to_monet = monet_generator(batch_photo_images)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Photo\")\n",
    "plt.imshow(batch_photo_images[0] * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Monet-esque Photo\")\n",
    "plt.imshow(to_monet[0] * 0.5 + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN model\n",
    "\n",
    "By learning the mapping relationship between two different domains, CycleGAN can transform image styles without using paired training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:06.726297Z",
     "iopub.status.busy": "2024-12-13T04:49:06.726004Z",
     "iopub.status.idle": "2024-12-13T04:49:06.738265Z",
     "shell.execute_reply": "2024-12-13T04:49:06.737395Z",
     "shell.execute_reply.started": "2024-12-13T04:49:06.726261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setting up the new GAN model\n",
    "\n",
    "class CycleGAN(Model):\n",
    "    def __init__(self, monet_generator, photo_generator, monet_discriminator, photo_discriminator, lambda_cycle=10, **kwargs):\n",
    "        super(CycleGAN, self).__init__(**kwargs)\n",
    "        self.m_gen = monet_generator\n",
    "        self.p_gen = photo_generator\n",
    "        self.m_disc = monet_discriminator\n",
    "        self.p_disc = photo_discriminator\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.d_loss_arr = []\n",
    "        self.g_loss_arr = []\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"lambda_cycle\": self.lambda_cycle,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, custom_objects=None):\n",
    "        # Create a new instance with the given configuration\n",
    "        model_instance = cls(\n",
    "            monet_generator=None,  # These will be set after loading weights\n",
    "            photo_generator=None,\n",
    "            monet_discriminator=None,\n",
    "            photo_discriminator=None,\n",
    "            lambda_cycle=config.get(\"lambda_cycle\", 10)\n",
    "        )\n",
    "        \n",
    "        return model_instance\n",
    "\n",
    "    def save_weights_separately(self, base_path):\n",
    "        # Save weights of each sub-model to separate files\n",
    "        m_gen_weights_path = f\"{base_path}_m_gen_weights.h5\"\n",
    "        p_gen_weights_path = f\"{base_path}_p_gen_weights.h5\"\n",
    "        m_disc_weights_path = f\"{base_path}_m_disc_weights.h5\"\n",
    "        p_disc_weights_path = f\"{base_path}_p_disc_weights.h5\"\n",
    "        print('path',m_gen_weights_path)\n",
    "        self.m_gen.save_weights(m_gen_weights_path)\n",
    "        self.p_gen.save_weights(p_gen_weights_path)\n",
    "        self.m_disc.save_weights(m_disc_weights_path)\n",
    "        self.p_disc.save_weights(p_disc_weights_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_weights_separately(cls, base_path, monet_generator, photo_generator, monet_discriminator, photo_discriminator, lambda_cycle=10):\n",
    "        # Load weights into each sub-model\n",
    "        m_gen_weights_path = f\"{base_path}_m_gen_weights.h5\"\n",
    "        p_gen_weights_path = f\"{base_path}_p_gen_weights.h5\"\n",
    "        m_disc_weights_path = f\"{base_path}_m_disc_weights.h5\"\n",
    "        p_disc_weights_path = f\"{base_path}_p_disc_weights.h5\"\n",
    "\n",
    "        monet_generator.load_weights(m_gen_weights_path)\n",
    "        photo_generator.load_weights(p_gen_weights_path)\n",
    "        monet_discriminator.load_weights(m_disc_weights_path)\n",
    "        photo_discriminator.load_weights(p_disc_weights_path)\n",
    "\n",
    "        # Initialize and return the CycleGAN instance with loaded weights\n",
    "        return cls(\n",
    "            monet_generator=monet_generator,\n",
    "            photo_generator=photo_generator,\n",
    "            monet_discriminator=monet_discriminator,\n",
    "            photo_discriminator=photo_discriminator,\n",
    "            lambda_cycle=lambda_cycle\n",
    "        )\n",
    "\n",
    "      \n",
    "    \n",
    "    def compile(self,m_gen_optimizer,p_gen_optimizer,m_disc_optimizer,p_disc_optimizer,gen_loss_fn,disc_loss_fn,cycle_loss_fn,identity_loss_fn):\n",
    "        super(CycleGAN, self).compile()\n",
    "        self.m_gen_optimizer = m_gen_optimizer\n",
    "        self.p_gen_optimizer = p_gen_optimizer\n",
    "        self.m_disc_optimizer = m_disc_optimizer\n",
    "        self.p_disc_optimizer = p_disc_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = cycle_loss_fn\n",
    "        self.identity_loss_fn = identity_loss_fn\n",
    "        \n",
    "    def train_step(self, batch_data):\n",
    "        # print(batch_data)\n",
    "        real_monet, real_photo = batch_data\n",
    "        # print(\"Real Monet shape:\", real_monet.shape)\n",
    "        # print(\"Real Photo shape:\", real_photo.shape)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # photo to monet back to photo\n",
    "            fake_monet = self.m_gen(real_photo, training=True)\n",
    "            cycled_photo = self.p_gen(fake_monet, training=True)\n",
    "\n",
    "            # monet to photo back to monet\n",
    "            fake_photo = self.p_gen(real_monet, training=True)\n",
    "            cycled_monet = self.m_gen(fake_photo, training=True)\n",
    "\n",
    "            # generating itself\n",
    "            same_monet = self.m_gen(real_monet, training=True)\n",
    "            same_photo = self.p_gen(real_photo, training=True)\n",
    "\n",
    "            # Using discriminator to check, inputing real images\n",
    "            disc_real_monet = self.m_disc(real_monet, training=True)\n",
    "            disc_real_photo = self.p_disc(real_photo, training=True)\n",
    "\n",
    "            # discriminator used to check, inputing fake images\n",
    "            disc_fake_monet = self.m_disc(fake_monet, training=True)\n",
    "            disc_fake_photo = self.p_disc(fake_photo, training=True)\n",
    "\n",
    "            # evaluates generator loss\n",
    "            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n",
    "            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n",
    "\n",
    "            # evaluates total cycle consistency loss\n",
    "            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
    "\n",
    "            # evaluates total generator loss\n",
    "            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n",
    "            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n",
    "\n",
    "            # evaluates discriminator loss\n",
    "            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n",
    "            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n",
    "\n",
    "        # Calculate the gradients for generator and discriminator\n",
    "        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n",
    "                                                  self.m_gen.trainable_variables)\n",
    "        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n",
    "                                                  self.p_gen.trainable_variables)\n",
    "\n",
    "        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n",
    "                                                      self.m_disc.trainable_variables)\n",
    "        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n",
    "                                                      self.p_disc.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n",
    "                                                 self.m_gen.trainable_variables))\n",
    "\n",
    "        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n",
    "                                                 self.p_gen.trainable_variables))\n",
    "\n",
    "        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n",
    "                                                  self.m_disc.trainable_variables))\n",
    "\n",
    "        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n",
    "                                                  self.p_disc.trainable_variables))\n",
    "        \n",
    "        self.g_loss_arr.append(total_monet_gen_loss + total_photo_gen_loss)\n",
    "        self.d_loss_arr.append(monet_disc_loss + photo_disc_loss)\n",
    "        return {\n",
    "            \"monet_gen_loss\": total_monet_gen_loss,\n",
    "            \"photo_gen_loss\": total_photo_gen_loss,\n",
    "            \"monet_disc_loss\": monet_disc_loss,\n",
    "            \"photo_disc_loss\": photo_disc_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:06.739390Z",
     "iopub.status.busy": "2024-12-13T04:49:06.739173Z",
     "iopub.status.idle": "2024-12-13T04:49:06.771786Z",
     "shell.execute_reply": "2024-12-13T04:49:06.771169Z",
     "shell.execute_reply.started": "2024-12-13T04:49:06.739368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def discriminator_loss(real, generated):\n",
    "        #The output of the real image should be close to 1 ( considered real)\n",
    "        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n",
    "        #The output of the generated image should be close to 0 ( considered fake)\n",
    "        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
    "\n",
    "        total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "        return total_disc_loss * 0.5\n",
    "    \n",
    "    def generator_loss(generated):\n",
    "        #The generator wants to produce images that the discriminator thinks are real ( the output is close to 1)\n",
    "        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n",
    "    \n",
    "    # LAMBDA-  The weight coefficient of the cyclic consistency loss, used to control the importance of this loss relative to other losses\n",
    "    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
    "        return LAMBDA * tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    \n",
    "    def identity_loss(real_image, same_image, LAMBDA):\n",
    "        return LAMBDA * 0.5 * tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:06.773095Z",
     "iopub.status.busy": "2024-12-13T04:49:06.772839Z",
     "iopub.status.idle": "2024-12-13T04:49:06.808298Z",
     "shell.execute_reply": "2024-12-13T04:49:06.807183Z",
     "shell.execute_reply.started": "2024-12-13T04:49:06.773069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:06.809993Z",
     "iopub.status.busy": "2024-12-13T04:49:06.809632Z",
     "iopub.status.idle": "2024-12-13T04:49:06.831943Z",
     "shell.execute_reply": "2024-12-13T04:49:06.831306Z",
     "shell.execute_reply.started": "2024-12-13T04:49:06.809952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    cycle_gan_model = CycleGAN(\n",
    "        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n",
    "    )\n",
    "\n",
    "    cycle_gan_model.compile(\n",
    "        m_gen_optimizer = monet_generator_optimizer,\n",
    "        p_gen_optimizer = photo_generator_optimizer,\n",
    "        m_disc_optimizer = monet_discriminator_optimizer,\n",
    "        p_disc_optimizer = photo_discriminator_optimizer,\n",
    "        gen_loss_fn = generator_loss,\n",
    "        disc_loss_fn = discriminator_loss,\n",
    "        cycle_loss_fn = calc_cycle_loss,\n",
    "        identity_loss_fn = identity_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:49:06.833177Z",
     "iopub.status.busy": "2024-12-13T04:49:06.832907Z",
     "iopub.status.idle": "2024-12-13T04:56:45.804501Z",
     "shell.execute_reply": "2024-12-13T04:56:45.803517Z",
     "shell.execute_reply.started": "2024-12-13T04:49:06.833151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with strategy.scope():\n",
    "#     history = cycle_gan_model.fit(tf.data.Dataset.zip((monet_data, photo_data)),\n",
    "#                         steps_per_epoch=(300//BATCH_SIZE),\n",
    "#                         epochs=EPOCHS,\n",
    "#                         verbose=2).history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:56:45.806856Z",
     "iopub.status.busy": "2024-12-13T04:56:45.806101Z",
     "iopub.status.idle": "2024-12-13T04:56:48.754787Z",
     "shell.execute_reply": "2024-12-13T04:56:48.753971Z",
     "shell.execute_reply.started": "2024-12-13T04:56:45.806809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(5, 2, figsize=(12, 12))\n",
    "for i, img in enumerate(photo_data.take(5)):\n",
    "    prediction = monet_generator(img, training=False)[0].numpy()\n",
    "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "\n",
    "    ax[i, 0].imshow(img)\n",
    "    ax[i, 1].imshow(prediction)\n",
    "    ax[i, 0].set_title(\"Input Photo\")\n",
    "    ax[i, 1].set_title(\"Monet-esque\")\n",
    "    ax[i, 0].axis(\"off\")\n",
    "    ax[i, 1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:57:51.994190Z",
     "iopub.status.busy": "2024-12-13T04:57:51.993883Z",
     "iopub.status.idle": "2024-12-13T04:57:52.001571Z",
     "shell.execute_reply": "2024-12-13T04:57:52.000638Z",
     "shell.execute_reply.started": "2024-12-13T04:57:51.994154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def create_folder(name,clearfile=False):\n",
    "    path = OUT_PUT_PATH+name\n",
    "    if(clearfile):\n",
    "        print(path)\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            print(name,' Deletion complete')\n",
    "        else:\n",
    "            print(name,' Originally null')\n",
    "            \n",
    "    os.makedirs(name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder('Model',False)\n",
    "\n",
    "with strategy.scope():\n",
    "    cycle_gan_model.save_weights_separately( OUT_PUT_PATH+'Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_cycle_gan_model = CycleGAN.load_weights_separately(\n",
    "    OUT_PUT_PATH+'Model/',\n",
    "    monet_generator=monet_generator,\n",
    "    photo_generator=photo_generator,\n",
    "    monet_discriminator=monet_discriminator,\n",
    "    photo_discriminator=photo_discriminator,\n",
    "    lambda_cycle=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:56:48.756613Z",
     "iopub.status.busy": "2024-12-13T04:56:48.755967Z",
     "iopub.status.idle": "2024-12-13T04:57:51.992554Z",
     "shell.execute_reply": "2024-12-13T04:57:51.991626Z",
     "shell.execute_reply.started": "2024-12-13T04:56:48.756576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "\n",
    "os.chdir(OUT_PUT_PATH)\n",
    "\n",
    "# 定义要压缩的文件夹名称和输出压缩文件的名称\n",
    "folder_to_compress = 'Model'  # 要压缩的文件夹名\n",
    "output_tar_file = 'model_folder.tar.gz'  # 输出的压缩文件名\n",
    "\n",
    "# 检查文件夹是否存在\n",
    "if not os.path.exists(folder_to_compress):\n",
    "    print(f\"Error: The folder '{folder_to_compress}' does not exist in the current directory.\")\n",
    "else:\n",
    "    # 创建压缩文件，包含整个文件夹及其内容\n",
    "    !tar -czf {output_tar_file} {folder_to_compress}\n",
    "    \n",
    "    # 提供下载链接\n",
    "    from IPython.display import FileLink\n",
    "    display(FileLink(output_tar_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:57:52.014223Z",
     "iopub.status.busy": "2024-12-13T04:57:52.013922Z",
     "iopub.status.idle": "2024-12-13T04:57:52.022914Z",
     "shell.execute_reply": "2024-12-13T04:57:52.022170Z",
     "shell.execute_reply.started": "2024-12-13T04:57:52.014198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import PIL\n",
    "# create_folder('images',True)\n",
    "# i = 1\n",
    "# for img in photo_data:\n",
    "#     for batch in range(BATCH_SIZE):\n",
    "#         img_g=img[batch:batch+1]\n",
    "#         # print(img_g.shape)\n",
    "#         prediction = monet_generator(img_g, training=False)[0].numpy()\n",
    "#         prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "#         im = PIL.Image.fromarray(prediction)\n",
    "#         im.save(\"images/\" + str(i) + \".jpg\")\n",
    "#         i += 1\n",
    "#         if i %100 ==0:\n",
    "#             print(i)\n",
    "#     if i >= 9000:  # 检查并退出外层循环\n",
    "#         break\n",
    "  \n",
    "\n",
    "# print('generator success')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:57:52.024730Z",
     "iopub.status.busy": "2024-12-13T04:57:52.024070Z",
     "iopub.status.idle": "2024-12-13T04:57:52.041443Z",
     "shell.execute_reply": "2024-12-13T04:57:52.040747Z",
     "shell.execute_reply.started": "2024-12-13T04:57:52.024692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_directory(folder_path, output_zip):\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, folder_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "zip_directory('/kaggle/working/images', '/kaggle/working/images.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T04:57:52.042853Z",
     "iopub.status.busy": "2024-12-13T04:57:52.042448Z",
     "iopub.status.idle": "2024-12-13T04:57:52.052977Z",
     "shell.execute_reply": "2024-12-13T04:57:52.052168Z",
     "shell.execute_reply.started": "2024-12-13T04:57:52.042826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('completion of task')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1475600,
     "sourceId": 21755,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30806,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
